{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "adca21a7",
   "metadata": {},
   "source": [
    "# Import Dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d6fac43",
   "metadata": {},
   "source": [
    "The framework we gonna use in this Notebook is stable-baseline which is a set of improved implementations of Reinforcement Learning (RL) algorithms based on OpenAI Baselines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7cd7ddb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym \n",
    "from stable_baselines3 import A2C\n",
    "from stable_baselines3.common.vec_env import VecFrameStack\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.env_util import make_atari_env\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a6bc570",
   "metadata": {},
   "source": [
    "# Load Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de0ba145",
   "metadata": {},
   "source": [
    "OpenAI gym provides us with many simulated environment to train our model on and in this notebook we gonna use the simulated environment from OpenAI called cartpole.\n",
    "\n",
    "How does the environment looks like?\n",
    "\n",
    "The game involves a wall of blocks, a ball, and a bat. If the ball hits a block, you get some score and the block is removed. You have to move the bat at the bottom of the screen to avoid the ball going out of play, which would cause you to lose one of the five lives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "db136e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "environment_name = \"Breakout-v0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "956c35db",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A.L.E: Arcade Learning Environment (version 0.7.4+069f8bd)\n",
      "[Powered by Stella]\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(environment_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94662137",
   "metadata": {},
   "source": [
    "Check the environment with random actions for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2d33b5ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ishan/.local/lib/python3.10/site-packages/gym/envs/atari/environment.py:267: UserWarning: \u001b[33mWARN: We strongly suggest supplying `render_mode` when constructing your environment, e.g., gym.make(ID, render_mode='human'). Using `render_mode` provides access to proper scaling, audio support, and proper framerates.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:1 Score:3.0\n",
      "Episode:2 Score:3.0\n",
      "Episode:3 Score:3.0\n",
      "Episode:4 Score:0.0\n",
      "Episode:5 Score:1.0\n"
     ]
    }
   ],
   "source": [
    "episodes = 5\n",
    "for episode in range(1, episodes+1):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    score = 0 \n",
    "    \n",
    "    while not done:\n",
    "        env.render()\n",
    "        action = env.action_space.sample()\n",
    "        n_state, reward, done, info = env.step(action)\n",
    "        score+=reward\n",
    "    print('Episode:{} Score:{}'.format(episode, score))\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a37c638",
   "metadata": {},
   "source": [
    "* Consider an episode as something like when an agent plays an entire game.\n",
    "\n",
    "* Here, we learn a loop of 5 episodes and reset the environment to original observation after every.\n",
    "\n",
    "* `env.render()` function is used to graphically represent the environment.\n",
    "\n",
    "* We then generate a random action from all the possible actions which is present in the sample space. \n",
    "\n",
    "* We then apply that action to the environment which was randomly generated, which will return four values. \n",
    "\n",
    "* The next set of observation, reward, the boolean value which says if the action is done or not and the info.\n",
    "\n",
    "* If the action is done, the while loop condition will be false and we will be moved on to next iteration."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d51e0c3c",
   "metadata": {},
   "source": [
    "# Vectorise Environment and Train Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78ad00ac",
   "metadata": {},
   "source": [
    "Create 4 different environment simultaneously to train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d964436b",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = make_atari_env('Breakout-v0', n_envs=4, seed=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ccab032",
   "metadata": {},
   "source": [
    "Vectorise the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f92a14a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = VecFrameStack(env, n_stack=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "07516ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_path = os.path.join('Training', 'Logs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "46f74977",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env in a VecTransposeImage.\n"
     ]
    }
   ],
   "source": [
    "model = A2C(\"CnnPolicy\", env, verbose=1, tensorboard_log=log_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4636ec4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.learn(total_timesteps=2000000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb70f6c5",
   "metadata": {},
   "source": [
    "# Load the pretrained Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da6c3600",
   "metadata": {},
   "source": [
    "I will load the model which is already trained on 2 Millions steps which is also available in Repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "31862f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = make_atari_env('Breakout-v0', n_envs=1, seed=0)\n",
    "env = VecFrameStack(env, n_stack=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "60a9475d",
   "metadata": {},
   "outputs": [],
   "source": [
    "a2c_path = os.path.join('A2C_2M_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4e2b2307",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrapping the env in a VecTransposeImage.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ishan/.local/lib/python3.10/site-packages/stable_baselines3/common/save_util.py:166: UserWarning: Could not deserialize object lr_schedule. Consider using `custom_objects` argument to replace this object.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model = A2C.load(a2c_path, env)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce0de6e8",
   "metadata": {},
   "source": [
    "# Evaluate and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f9c650a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(23.1, 9.782126558167198)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_policy(model, env, n_eval_episodes=10, render=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1520d65d",
   "metadata": {},
   "source": [
    "The model worked better. We get average score of 23 and STD of 9 which is a good result because it's trained in 2 millions steps. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
